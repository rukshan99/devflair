{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ec88de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './reut.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11764/1787544760.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m \u001b[0mref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11764/1787544760.py\u001b[0m in \u001b[0;36mtf\u001b[1;34m(n, source)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mall_bodies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_reuters_news\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mNO_OF_ENTRIES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11764/1787544760.py\u001b[0m in \u001b[0;36mextract_reuters_news\u001b[1;34m(path_file)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mextract_reuters_news\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_file\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mall_bodies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# replace body with content, otherwise bs4 wont find any body other then main body\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './reut.html'"
     ]
    }
   ],
   "source": [
    "import re, nltk, bs4\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import scipy as sp\n",
    "from scipy.sparse import csr_matrix as csr\n",
    "import random\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "file_path = 'C:\\\\path\\\\to\\\\both_files\\\\'\n",
    "\n",
    "def tf(n=5, source='reuters'):\n",
    "    # define number of relevant words n\n",
    "    nltk.download('stopwords')\n",
    "    punc = list(punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    if source == 'reuters':\n",
    "        file_path = './reut.html'\n",
    "        \n",
    "\n",
    "        def extract_reuters_news(path_file):\n",
    "            file = open(path_file , 'r').read()\n",
    "            soup = bs4.BeautifulSoup(file)\n",
    "            all_bodies = [el.text for el in soup.find_all('content')] # replace body with content, otherwise bs4 wont find any body other then main body\n",
    "            return all_bodies\n",
    "\n",
    "        data = extract_reuters_news(file_path)\n",
    "        NO_OF_ENTRIES = len(data)\n",
    "\n",
    "    if source == 'imdb':\n",
    "        data = pd.read_csv('./imdb.csv')\n",
    "        NO_OF_ENTRIES = len(data) //2\n",
    "        NO_OF_ENTRIES\n",
    "        data = data.review.iloc[:NO_OF_ENTRIES]\n",
    "\n",
    "    ## GLOBAL DICTS\n",
    "    ## countains overall count of the term among all documents\n",
    "    maximum_per_document = defaultdict(int) # maximum a term occurs in one doc. denominator for first equation\n",
    "    number_docs_containing_term = defaultdict(int) ## How many documents contain a term --> denominator for second equation\n",
    "\n",
    "    # bow_count will clean the input, create sets for every sentence and return a dict {word:count} & int(maximum count per doc)\n",
    "    def bow_count(sentences):\n",
    "        new_sentence = ''\n",
    "        sentences = re.sub(r'<\\s*br\\s*\\/s*>', '', sentences)\n",
    "        sentences = re.sub(r'\\n>', ' ', sentences)\n",
    "        sentences = re.sub(r'\\s+', ' ', sentences)\n",
    "        sentences = re.sub(r'\\.+\\s*', '.', sentences)\n",
    "        sentences = re.sub(r'who\\'ll', 'who will', sentences)\n",
    "        sentences = re.sub(r'[IiyouYousheSHE]\\'ll', 'i will', sentences)\n",
    "        sentences = re.sub(r'[wW]ouldn\\'t', 'would not', sentences)\n",
    "        sentences = re.sub(r'[mM]mustn\\'t', 'must not', sentences)\n",
    "        sentences = re.sub(r'[tT]hat\\'s', 'that is', sentences)\n",
    "\n",
    "        for el in sentences:\n",
    "            if el.isspace() or el.isalpha() or el == '.': #or el.isnumeric():\n",
    "                new_sentence += el.lower()\n",
    "\n",
    "        new_sentences = new_sentence.split('.')\n",
    "        new_sentences = [set(e for e in el.split() if e not in stop_words) for el in new_sentence.split('.')]\n",
    "        temp_set = set()\n",
    "        temp_count = defaultdict(int)\n",
    "\n",
    "        for el in new_sentences:\n",
    "            for l in el:\n",
    "\n",
    "                temp_count[l] += 1\n",
    "                temp_set.add(l)\n",
    "\n",
    "        doc_max_term_count = [v for k,v in sorted(temp_count.items(), key= lambda x : x[1], reverse=True)][0]\n",
    "\n",
    "        for term in temp_set:\n",
    "            number_docs_containing_term[term] += 1\n",
    "\n",
    "        return temp_count, doc_max_term_count ## returning a list of sets, where every set is a sentence\n",
    "\n",
    "    docs = []\n",
    "    for i,doc in enumerate(data):\n",
    "        counted_terms, m = bow_count(doc)\n",
    "        maximum_per_document[i] = m\n",
    "        docs.append(counted_terms)\n",
    "\n",
    "    def get_tf_idf(w,doc_index):\n",
    "        tf_idf = {}\n",
    "        tf = {}\n",
    "\n",
    "        for k,v in w.items():\n",
    "            tf[k] = v / maximum_per_document[doc_index]\n",
    "            ni = number_docs_containing_term[k]\n",
    "            from math import log\n",
    "            idf = log(NO_OF_ENTRIES / ni)\n",
    "            tf_idf[k] = tf[k] * idf\n",
    "\n",
    "        return tf_idf\n",
    "\n",
    "    result = []\n",
    "    words_vector = set()\n",
    "\n",
    "    for ind, words in enumerate(docs):\n",
    "        ranked_words = get_tf_idf(words, ind)\n",
    "        top_n = {k:v for k,v in sorted(ranked_words.items(), key=lambda x: (-x[1]) )[:n] }\n",
    "        result.append(top_n)\n",
    "        top_set = set([el for el in top_n.keys()])\n",
    "        words_vector |= top_set\n",
    "\n",
    "    all_word_vector = np.zeros(len(words_vector))\n",
    "\n",
    "    ## global list that will then be stacked to sparse matrix\n",
    "    similarity_to_stack = []\n",
    "    ## create a similariy vector of all words -> which is then used to create per-result-datapoint-vectors --> stacking those to matrix\n",
    "    def similarity_vector(words):\n",
    "        doc_vec = all_word_vector.copy()\n",
    "\n",
    "        for i,word in enumerate(words_vector):\n",
    "            if word in words:\n",
    "                doc_vec[i] = 1\n",
    "\n",
    "        doc_vec_norm = np.linalg.norm(doc_vec)\n",
    "        doc_vec /= doc_vec_norm\n",
    "\n",
    "        return doc_vec # which is a vector that is normalized and can be compared to all others\n",
    "\n",
    "    # iterate over all entries in result (dictonary with n entries of top words)\n",
    "    for progress,r in enumerate(result):\n",
    "        similarity_to_stack.append(similarity_vector(list(r.keys())))\n",
    "        if progress%1000 == 0:\n",
    "            print(progress, ' records analysed.')\n",
    "\n",
    "    # stack all results similarity vectors to one matrix\n",
    "    m = csr(np.vstack(similarity_to_stack))\n",
    "    m.shape\n",
    "\n",
    "    # print the stacked matrix:\n",
    "    # m: number of datapoints, n: number of words in all_word_vector\n",
    "    plt.spy(m, marker='.', markersize=1)\n",
    "\n",
    "    # create a similarity vector, by multiplying each element with all others\n",
    "    ref = m.dot(m.T).toarray()\n",
    "\n",
    "    return ref, data\n",
    "\n",
    "ref,data = tf()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# quick & dirty: identify similar articles/blogs/reviews\n",
    "for ind,ary in enumerate(ref):\n",
    "    for i,el in enumerate(ary):\n",
    "        if el > .6 and ind != i :\n",
    "            print(ind, ' and ', i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd25ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
